---
title: 'Notes on "Are All Good Word Vector Spaces Isomorphic"'
author: "Aathreya Kadambi"
date: "April 16, 2024"
header-includes:
- \usepackage{NotesTemplateF}
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_folding: show
  pdf_document: default
---
<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
    body { 
      background-color: rgb(224, 244, 255); 
      font-family: Montserrat;
    }
    .toc-content { 
      height: 100%;
      background-color: white; 
      mathjax: "default";
    }
</style>

## Why I Decided to Read this Paper

Recently, I've been in a project that requires us to develop a sense of what the output for a certain input looks like. I guess there are a lot of projects like that, so to be more precise, if we let $\mathcal{S}_{\mathcal{A}}$ be the collection of strings based on some alphabet $\mathcal{A}$, there is some map $f : \mathcal{S}_{\mathcal{A}} \rightarrow \mathcal{S}_{\mathcal{A}}$ which is sort of open or continuous in some sense, meaning that it preserves information in some way. In other words, things that are similar will get mapped to things that are similar. I want to find the best approximation for $f$.

Anyway, I decided to read the paper.

## Laplacian Matrix of a Graph

Part of understanding how similar embeddings are is understanding the idea of how close a map is to an isomorphism (I had originally been thinking isometries, but this works too). One way to "quantify isomorphism" uses the notion of a Laplacian matrix. 


